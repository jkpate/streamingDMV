This repository contains implementations of the Dependency Model with Valence
(Klein and Manning, 2005) and variants, together with implementations of three
scalable algorithms for variational Bayesian inference. This software was used
to produce the results in:

Pate, J K and Johnson, M (2016). Grammar induction from (lots of) words alone.
In proceedings of COLING 2016.

To compile the software, install the simple-build-tool for Scala, and run:

$ sbt assembly


This produces a single jar file under target/scala-2.11/streamingDMV.jar.

Training and evaluation sentences should be provided in plain text files with
one sentence per line.  The line should begin with a unique label for the
sentence, a space, and then the tokens of the sentence, separated by spaces. For
example:

train.utt1: colorless green ideas sleep furiously
train.utt2: the cow jumped over the moon


run.scala contains code for running all the models. The results in Pate and
Johnson (2016) were obtained with TopDownDMVParser (this produces identical
results to OriginalDMVParser, but I have not implemented the scalable algorithms
for the OriginalDMVParser version).

To run batch VB with TopDownParser that terminates when the log-likelihood has
changed less than 0.001%, use the following command:

java -jar target/scala-2.11/streamingDMV.jar streamingDMV.run \
  -trainStrings <training strings file> \
  -testStrings <test strings file> \
  -parserType TopDownDMVParser \
  -batchVB \
  -miniBatchSize <number of training strings> \
  -incConvergence 1E-5

To use a version of the harmonic initialization from Klein and Manning (2004),
additionally pass the flag: -harmonicCorpusInit

To obtain learning curves for Batch VB, repeatedly run the above command with
larger arguments for miniBatchSize, and provide the same random seed with
-randomSeed <number>. A consistent random seed ensures that larger training sets
contain all of the strings from the smaller training sets.


Pate and Johnson (2016) additionally explore stochastic VB (Hoffman et al.,
2013), collapsed VB (Teh et al 2007; Asuncion et al., 2009; Wang and Blunsom,
2013), and streaming VB (Broderick et al., 2013).

To run stochastic VB, replace the -batchVB with -stochastic VB, optionally
provide -tau and -kappa for the learning rate schedule), and adjust the argument
to -miniBatchSize to be the size of your minibatches. Additionally, set the
number of epochs with -epochCount <number>.

To run collapsed VB, replace -batchVB with -collapsedVB, and remove
-miniBatchSize (or set it to 1).  Additionally, set the number of epochs with
-epochs <number>. Be warned that collapsed VB maintains a miniature grammar of
counts for every sentence, and so has substantial memory requirements.

To run streaming VB, delete -batchVB. We additionally found good performance by
using a large initial minibatch of 10,000 sentences, and using subsequent
minibatches of a single sentence. This can be accomplished by using
-initialMiniBatchSize 10000 and -miniBatchSize 1. You can control the number of
iterations of VB per sentence with -incIters <number>. We found 1 iteration to
work well.

To obtain learning curves for the scalable algorithms, use -evalEvery <number>.
After processing evalEvery sentences, parameters will be "frozen" and the parser
will print out the Viterbi dependency parse for the test set. Dependency parses
are printed as a vector of zero-based head indices, with root set to the length
of the string. The following output indicates that the second word is the root,
and the first and last word depend on the second word:

test.utt1: [ 1, 2, 1 ]

Additionally, the parser prints out the predictive log probability of the test
set.

You can evaluate on a log scale (every 10 sentences through the first 100
sentences, then every 100 sentences through the first 1000 sentences, and so on)
by passing -logEvalRate.

The results for Pate and Johnson (2016) were obtained with the following
commands, with random seeds of 15, 17, 19, 21, and 23. In all cases, add the
-harmonicCorpusInit flag to use a harmonic initialization.


Batch VB:

java -Xmx6g -jar target/scala-2.11/streamingDMV.jar streamingDMV.run \
  -trainStrings <training strings file> \
  -testStrings <test strings file> \
  -parserType TopDownDMVParser \
  -batchVB \
  -randomSeed <#> \
  -miniBatchSize <number of training strings> \
  -incConvergence 1E-5


Stochastic VB:

java -Xmx6g -jar target/scala-2.11/streamingDMV.jar streamingDMV.run \
  -trainStrings <training strings file> \
  -testStrings <test strings file> \
  -parserType TopDownDMVParser \
  -stochasticVB \
  -randomSeed <#> \
  -miniBatchSize 10000 \
  -kappa 0.9 \
  -tau 1 \
  -evalEvery 10000 \
  -logEvalRate \
  -epochCount 15

Collapsed VB:

java -Xmx60g -jar target/scala-2.11/streamingDMV.jar streamingDMV.run \
  -trainStrings <training strings file> \
  -testStrings <test strings file> \
  -parserType TopDownDMVParser \
  -collapsedVB \
  -randomSeed <#> \
  -miniBatchSize 1 \
  -evalEvery 10 \
  -logEvalRate \
  -epochCount 15


Streaming VB:

java -Xmx6g -jar target/scala-2.11/streamingDMV.jar streamingDMV.run \
  -trainStrings <training strings file> \
  -testStrings <test strings file> \
  -parserType TopDownDMVParser \
  -randomSeed <#> \
  -initialMiniBatchSize 10000 \
  -miniBatchSize 1 \
  -incIters 1 \
  -evalEvery 10 \
  -logEvalRate


The local/bin/ directory contains perl scripts for plotting learning curves from
the produced output.


This repository contains several other implementations. The ThreeValence,
FourValence, and FiveValence parsers are similar to the DMV, except they have
additional levels of valence for the stop decisions. In preliminary experiments
using streaming VB, the additional levels of valence produced an additional 2
points in directed attachment accuracy, although the advantage of five over four
levels was much smaller than three over two levels. The NoValence parser does
not distinguish valence in the stop decisions, and performs poorly.


There is also an implementation of particle filters, with optional rejuvenation,
but these performed poorly.




